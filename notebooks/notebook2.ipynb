{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6058f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "import numpy as np\n",
    "from stable_baselines3 import SAC, HerReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8122981",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGES = {\n",
    "    '1' : 'HOVER', \n",
    "    '2' : 'GRASP', \n",
    "    '3' : 'PLACE', \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9ff45fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'observation': array([ 1.34193475e+00,  7.49101049e-01,  5.34725189e-01,  1.37882812e+00,\n",
       "          6.48572855e-01,  4.24892225e-01,  3.68933623e-02, -1.00528194e-01,\n",
       "         -1.09832964e-01,  3.83587563e-06,  6.44539257e-08, -8.04518216e-19,\n",
       "          1.42324495e-16,  1.16233816e-16,  5.26138825e-06,  7.50031136e-08,\n",
       "         -2.45923720e-05, -1.87673947e-18, -2.11089242e-16,  1.88168714e-18,\n",
       "         -5.26138825e-06, -7.50031136e-08,  2.46214240e-05, -9.88156877e-07,\n",
       "          7.65319420e-08]),\n",
       "  'achieved_goal': array([1.37882812, 0.64857285, 0.42489222]),\n",
       "  'desired_goal': array([1.37918241, 0.64031931, 0.42489222])},\n",
       " {})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('FetchPickAndPlace-v4', render_mode='human')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "69f8cb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__annotations__\n",
      "__class__\n",
      "__class_getitem__\n",
      "__delattr__\n",
      "__dict__\n",
      "__dir__\n",
      "__doc__\n",
      "__enter__\n",
      "__eq__\n",
      "__exit__\n",
      "__format__\n",
      "__ge__\n",
      "__getattribute__\n",
      "__getstate__\n",
      "__gt__\n",
      "__hash__\n",
      "__init__\n",
      "__init_subclass__\n",
      "__le__\n",
      "__lt__\n",
      "__module__\n",
      "__ne__\n",
      "__new__\n",
      "__orig_bases__\n",
      "__parameters__\n",
      "__reduce__\n",
      "__reduce_ex__\n",
      "__repr__\n",
      "__setattr__\n",
      "__sizeof__\n",
      "__str__\n",
      "__subclasshook__\n",
      "__weakref__\n",
      "_action_space\n",
      "_cached_spec\n",
      "_elapsed_steps\n",
      "_max_episode_steps\n",
      "_metadata\n",
      "_np_random\n",
      "_np_random_seed\n",
      "_observation_space\n",
      "_saved_kwargs\n",
      "action_space\n",
      "class_name\n",
      "close\n",
      "env\n",
      "get_wrapper_attr\n",
      "has_wrapper_attr\n",
      "metadata\n",
      "np_random\n",
      "np_random_seed\n",
      "observation_space\n",
      "render\n",
      "render_mode\n",
      "reset\n",
      "set_wrapper_attr\n",
      "spec\n",
      "step\n",
      "unwrapped\n",
      "wrapper_spec\n"
     ]
    }
   ],
   "source": [
    "for name in dir(env):\n",
    "    print(name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d65e72fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Methods only:\n",
      "__class__\n",
      "__class_getitem__\n",
      "__delattr__\n",
      "__dir__\n",
      "__enter__\n",
      "__eq__\n",
      "__exit__\n",
      "__format__\n",
      "__ge__\n",
      "__getattribute__\n",
      "__getstate__\n",
      "__gt__\n",
      "__hash__\n",
      "__init__\n",
      "__init_subclass__\n",
      "__le__\n",
      "__lt__\n",
      "__ne__\n",
      "__new__\n",
      "__reduce__\n",
      "__reduce_ex__\n",
      "__repr__\n",
      "__setattr__\n",
      "__sizeof__\n",
      "__str__\n",
      "__subclasshook__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't access `_np_random` of a wrapper, use `.unwrapped._np_random` or `.np_random`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMethods only:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(env):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     attr = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(attr):\n\u001b[32m      6\u001b[39m         \u001b[38;5;28mprint\u001b[39m(name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/LLM-Robotic-Arm/.venv/lib/python3.12/site-packages/gymnasium/core.py:526\u001b[39m, in \u001b[36mWrapper._np_random\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    520\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_np_random\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    522\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"This code will never be run due to __getattr__ being called prior this.\u001b[39;00m\n\u001b[32m    523\u001b[39m \n\u001b[32m    524\u001b[39m \u001b[33;03m    It seems that @property overwrites the variable (`_np_random`) meaning that __getattr__ gets called with the missing variable.\u001b[39;00m\n\u001b[32m    525\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m    527\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt access `_np_random` of a wrapper, use `.unwrapped._np_random` or `.np_random`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    528\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: Can't access `_np_random` of a wrapper, use `.unwrapped._np_random` or `.np_random`."
     ]
    }
   ],
   "source": [
    "\n",
    "# Get just the methods\n",
    "print(\"\\nMethods only:\")\n",
    "for name in dir(env):\n",
    "    attr = getattr(env, name)\n",
    "    if callable(attr):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd059aab",
   "metadata": {},
   "source": [
    "### finding the range of goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0b3533a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_x, goal_y, goal_z = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "17109459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8421187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    obs, _ = env.reset()\n",
    "    goal_x.append(obs['desired_goal'][0])\n",
    "    goal_y.append(obs['desired_goal'][1])\n",
    "    goal_z.append(obs['desired_goal'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "84cff141",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c3757e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(1.1919517529646197),\n",
       " np.float64(0.5991066228570885),\n",
       " np.float64(0.4248922232488584),\n",
       " np.float64(1.491934091203951),\n",
       " np.float64(0.8990975732816955),\n",
       " np.float64(0.8748198015890661))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(goal_x), min(goal_y), min(goal_z), max(goal_x), max(goal_y), max(goal_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8357a8b7",
   "metadata": {},
   "source": [
    "from code:\n",
    "object spawn center ≈ [1.25, 0.53, 0.4]\n",
    "obj_range=0.15\n",
    "target_range=0.15\n",
    "\n",
    "therefore, \n",
    "x ∈ [1.25 - 0.15, 1.25 + 0.15] =>  [1.10, 1.40]\n",
    "y ∈ [0.53 - 0.15, 0.53 + 0.15] =>  [0.38, 0.68]\n",
    "0.4 (ground clearance) -> 0.4 + random(0, 0.4) = 0.4 -> 0.8\n",
    "\n",
    "\n",
    "x ∈ [1.10, 1.40]\n",
    "y ∈ [0.38, 0.68]\n",
    "z ≈ 0.42 (on the table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e7017f",
   "metadata": {},
   "source": [
    "final:\n",
    "x in [1.20, 1.48]\n",
    "y in [0.62, 0.88]\n",
    "z in [0.425, 0.875]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "62bef057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'gymnasium_robotics.envs.fetch.pick_and_place.MujocoFetchPickAndPlaceEnv'>, <class 'gymnasium_robotics.envs.fetch.fetch_env.MujocoFetchEnv'>, <class 'gymnasium_robotics.envs.fetch.fetch_env.get_base_fetch_env.<locals>.BaseFetchEnv'>, <class 'gymnasium_robotics.envs.robot_env.MujocoRobotEnv'>, <class 'gymnasium_robotics.envs.robot_env.BaseRobotEnv'>, <class 'gymnasium_robotics.core.GoalEnv'>, <class 'gymnasium.core.Env'>, <class 'typing.Generic'>, <class 'gymnasium.utils.ezpickle.EzPickle'>, <class 'object'>)\n"
     ]
    }
   ],
   "source": [
    "from gymnasium_robotics.envs.fetch.pick_and_place import MujocoFetchPickAndPlaceEnv\n",
    "import inspect\n",
    "\n",
    "print(MujocoFetchPickAndPlaceEnv.__mro__)  # shows the parent classes\n",
    "\n",
    "# import gymnasium_robotics.envs.fetch.fetch_env.MujocoFetchEnv as base_fetch\n",
    "# print(inspect.getsource(base_fetch.BaseFetchEnv.reset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec810d1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TimeLimit' object has no attribute 'observation'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobservation\u001b[49m.desired_goal\n",
      "\u001b[31mAttributeError\u001b[39m: 'TimeLimit' object has no attribute 'observation'"
     ]
    }
   ],
   "source": [
    "class CustomObs(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "    def set_goal\n",
    "        \n",
    "    def reset(self, x, y, z):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adc4656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_custom_goal(env, obs, target_pos):\n",
    "    \"\"\"\n",
    "    Updates the observation for the agent AND moves the visual red dot.\n",
    "    \n",
    "    Args:\n",
    "        env: The Gymnasium environment\n",
    "        obs: The current observation dictionary\n",
    "        target_pos: [x, y, z] numpy array of the new target\n",
    "        \n",
    "    Returns:\n",
    "        The modified observation dictionary\n",
    "    \"\"\"\n",
    "    # 1. Hack the Agent's Input\n",
    "    obs['desired_goal'] = target_pos.copy()\n",
    "    \n",
    "    # 2. Hack the Visuals (The Red Dot)\n",
    "    try:\n",
    "        # Note: Different versions of Gym/Mujoco access data differently.\n",
    "        # This works for the standard Gymnasium-Robotics + Mujoco setup.\n",
    "        site_id = env.unwrapped.model.site(\"target0\").id\n",
    "        env.unwrapped.model.site_pos[site_id] = target_pos\n",
    "        env.unwrapped.data.forward()\n",
    "    except:\n",
    "        # If visuals fail (e.g. headless mode), just ignore it. \n",
    "        # The robot will still work.\n",
    "        pass\n",
    "\n",
    "    return obs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e92b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca60db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_from_slider = np.array([1.3, 0.6, 0.25])\n",
    "obs = set_custom_goal(env, obs, target_from_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c80025b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"FetchReach-v4\")\n",
    "obs, info = env.reset()\n",
    "\n",
    "env.goal = np.array([1.25, 0.75, 0.5])\n",
    "# Also update achieved + desired dict for agent\n",
    "obs[\"desired_goal\"] = env.goal.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d74c857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'observation': array([ 1.32490612e+00,  7.52627706e-01,  5.64787106e-01,  1.20922710e+00,\n",
       "          8.91670395e-01,  4.24892240e-01, -1.15679020e-01,  1.39042689e-01,\n",
       "         -1.39894866e-01,  1.05613321e-05,  1.04728067e-04, -1.08868783e-18,\n",
       "          3.34089087e-17,  1.19630606e-16,  1.50461182e-02, -2.96663660e-03,\n",
       "         -2.58884151e-02, -1.89976501e-18, -5.50611670e-17, -4.93610409e-19,\n",
       "         -1.50461182e-02,  2.96663660e-03,  2.58884222e-02,  5.28888059e-05,\n",
       "          5.01231694e-05]),\n",
       "  'achieved_goal': array([1.2092271 , 0.89167039, 0.42489224]),\n",
       "  'desired_goal': array([1.25786782, 0.8720507 , 0.42489222])},\n",
       " np.float32(-1.0),\n",
       " False,\n",
       " False,\n",
       " {'is_success': np.float32(0.0)})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d2f7ba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3d82a2f",
   "metadata": {},
   "source": [
    "#### by modifying subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aff536b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium_robotics.envs.fetch.pick_and_place import MujocoFetchPickAndPlaceEnv\n",
    "\n",
    "class FetchPickAndPlaceCustomGoal(MujocoFetchPickAndPlaceEnv):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.current_goal = None\n",
    "\n",
    "    def set_goal(self, goal: np.ndarray):\n",
    "        self.current_goal = goal.copy()\n",
    "\n",
    "    def _sample_goal(self):\n",
    "        assert self.current_goal is not None, \"Call set_goal before reset\"\n",
    "        return self.current_goal.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8bb8097a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 goal: [1.25 0.75 0.5 ]\n"
     ]
    }
   ],
   "source": [
    "env = FetchPickAndPlaceCustomGoal(render_mode='human')\n",
    "\n",
    "# you do some stuff here: ask LLM, look at an image, etc.\n",
    "# you only know the point at runtime\n",
    "goal1 = np.array([1.25, 0.75, 0.50])\n",
    "\n",
    "env.set_goal(goal1)          # set it right before reset\n",
    "obs, info = env.reset()\n",
    "print(\"Episode 1 goal:\", obs[\"desired_goal\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53c78039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 goal: [1.5  0.75 0.45]\n"
     ]
    }
   ],
   "source": [
    "goal1 = np.array([1.5, 0.75, 0.45])\n",
    "\n",
    "env.set_goal(goal1)          # set it right before reset\n",
    "obs, info = env.reset()\n",
    "print(\"Episode 1 goal:\", obs[\"desired_goal\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b8dd966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'observation': array([ 1.35176167e+00,  7.63733241e-01,  5.87246855e-01,  1.23791458e+00,\n",
       "          7.98351906e-01,  4.24892243e-01, -1.13847084e-01,  3.46186648e-02,\n",
       "         -1.62354611e-01,  4.00974884e-02,  4.17101562e-02,  1.89022274e-17,\n",
       "          1.88849544e-17,  1.19287698e-16, -6.43170103e-04, -2.07898149e-02,\n",
       "         -1.94346395e-02, -1.20629910e-17, -2.61025536e-17, -3.39347966e-18,\n",
       "          6.43170103e-04,  2.07898149e-02,  1.94346412e-02,  6.98444984e-02,\n",
       "          7.09726124e-02]),\n",
       "  'achieved_goal': array([1.23791458, 0.79835191, 0.42489224]),\n",
       "  'desired_goal': array([1.47198361, 0.68021574, 0.52509059])},\n",
       " np.float32(-1.0),\n",
       " False,\n",
       " False,\n",
       " {'is_success': np.float32(0.0)})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(env.action_space.sample())     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6d27daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e40e090",
   "metadata": {},
   "source": [
    "#### by adding wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f475a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "class CustomGoalWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.current_goal = None\n",
    "\n",
    "    def set_goal(self, goal):\n",
    "        \"\"\"Set goal for the *next* reset.\"\"\"\n",
    "        goal = np.asarray(goal, dtype=np.float32)\n",
    "        self.current_goal = goal\n",
    "\n",
    "    def _apply_goal(self, obs):\n",
    "        \"\"\"Update internal goal, desired_goal, and the red target site.\"\"\"\n",
    "        if self.current_goal is None:\n",
    "            return obs\n",
    "\n",
    "        g = self.current_goal.copy()\n",
    "\n",
    "        # goal used by reward and HER\n",
    "        self.env.goal = g\n",
    "\n",
    "        # move the red target site in MuJoCo so the dot moves\n",
    "        self.env.model.site_pos[self.env.target_site_id] = g\n",
    "\n",
    "        # update observation so the agent sees the new goal\n",
    "        obs[\"desired_goal\"] = g\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def reset(self, *, goal=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Optional argument:\n",
    "            goal: np.array(3,)    desired target position for the object\n",
    "        \"\"\"\n",
    "        if goal is not None:\n",
    "            self.set_goal(goal)\n",
    "\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        obs = self._apply_goal(obs)\n",
    "        return obs, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGoalWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.current_goal = None\n",
    "        \n",
    "    def set_goal(self, goal):\n",
    "        self.current_goal = goal\n",
    "        \n",
    "    def _apply_goal(self, obs):\n",
    "        if self.current_goal is not None:\n",
    "            g = self.current_goal.copy()\n",
    "            \n",
    "            self.env.goal = g\n",
    "            self.env.model.site_pos[self.env.target_site_id] = g\n",
    "            obs['desired_goal'] = g\n",
    "            \n",
    "        return obs\n",
    "\n",
    "    def reset(self, goal, **kwargs):\n",
    "        if self.goal is not None\n",
    "            self.set_goal(goal)\n",
    "        \n",
    "        obs, info = self.reset(**kwargs):\n",
    "        obs = self._apply_goal(obs)\n",
    "        \n",
    "        return obs, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc82b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "069c0a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "from gymnasium_robotics.envs.fetch.fetch_env import MujocoFetchEnv\n",
    "\n",
    "# Build the path to the multi-object XML inside gymnasium_robotics\n",
    "GYMROB_ROOT = os.path.dirname(gymnasium_robotics.__file__)\n",
    "MODEL_XML_PATH = os.path.join(\n",
    "    GYMROB_ROOT,\n",
    "    \"envs\",\n",
    "    \"assets\",\n",
    "    \"fetch\",\n",
    "    \"multi_pick_and_place.xml\",\n",
    ")\n",
    "\n",
    "\n",
    "class MultiObjectFetchPickAndPlaceEnv(MujocoFetchEnv):\n",
    "    def __init__(self, reward_type: str = \"sparse\", **kwargs):\n",
    "        # You must set initial_qpos for every object's free joint\n",
    "        initial_qpos = {\n",
    "            \"robot0:slide0\": 0.405,\n",
    "            \"robot0:slide1\": 0.48,\n",
    "            \"robot0:slide2\": 0.0,\n",
    "            # main object (object0) roughly like default\n",
    "            \"object0:joint\": [1.25, 0.70, 0.42, 1.0, 0.0, 0.0, 0.0],\n",
    "            # extra objects scattered on the table\n",
    "            \"object1:joint\": [1.30, 0.80, 0.42, 1.0, 0.0, 0.0, 0.0],\n",
    "            \"object2:joint\": [1.35, 0.60, 0.42, 1.0, 0.0, 0.0, 0.0],\n",
    "            \"object3:joint\": [1.40, 0.75, 0.42, 1.0, 0.0, 0.0, 0.0],\n",
    "            \"object4:joint\": [1.45, 0.65, 0.42, 1.0, 0.0, 0.0, 0.0],\n",
    "        }\n",
    "\n",
    "        super().__init__(\n",
    "            model_path=MODEL_XML_PATH,\n",
    "            has_object=True,\n",
    "            block_gripper=False,\n",
    "            n_substeps=20,\n",
    "            gripper_extra_height=0.2,\n",
    "            target_in_the_air=True,\n",
    "            target_offset=0.0,\n",
    "            obj_range=0.15,\n",
    "            target_range=0.15,\n",
    "            distance_threshold=0.05,\n",
    "            initial_qpos=initial_qpos,\n",
    "            reward_type=reward_type,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c839eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multi_fetch_env import MultiObjectFetchPickAndPlaceEnv\n",
    "\n",
    "env = MultiObjectFetchPickAndPlaceEnv(render_mode=\"human\")\n",
    "obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9da9428",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79f5ecfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'observation': array([ 1.34193475e+00,  7.49101049e-01,  5.34725189e-01,  1.24736527e+00,\n",
       "          8.68241707e-01,  4.24892240e-01, -9.45694868e-02,  1.19140658e-01,\n",
       "         -1.09832949e-01,  3.83587563e-06,  6.44539257e-08, -3.83126585e-18,\n",
       "         -1.91079569e-16, -5.11864729e-14,  5.26138825e-06,  7.50031136e-08,\n",
       "         -2.46153366e-05, -1.29834876e-17,  2.94396616e-16, -6.32178674e-19,\n",
       "         -5.26138825e-06, -7.50031136e-08,  2.46214240e-05, -9.88156877e-07,\n",
       "          7.65319420e-08]),\n",
       "  'achieved_goal': array([1.24736527, 0.86824171, 0.42489224]),\n",
       "  'desired_goal': array([1.43710006, 0.89503746, 0.42489224])},\n",
       " {})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b34283ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'observation': array([ 1.34193475e+00,  7.49101049e-01,  5.34725189e-01,  1.20239469e+00,\n",
       "          8.03342837e-01,  4.24892240e-01, -1.39540063e-01,  5.42417889e-02,\n",
       "         -1.09832949e-01,  3.83587563e-06,  6.44539257e-08, -3.83126585e-18,\n",
       "         -1.91079569e-16, -5.11864729e-14,  5.26138825e-06,  7.50031136e-08,\n",
       "         -2.46153366e-05, -1.29834876e-17,  2.94396616e-16, -6.32178674e-19,\n",
       "         -5.26138825e-06, -7.50031136e-08,  2.46214240e-05, -9.88156877e-07,\n",
       "          7.65319420e-08]),\n",
       "  'achieved_goal': array([1.20239469, 0.80334284, 0.42489224]),\n",
       "  'desired_goal': array([1.2633913 , 0.78164566, 0.51935622])},\n",
       " {})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db89ed87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'observation': array([ 1.34193475e+00,  7.49101049e-01,  5.34725189e-01,  1.20576514e+00,\n",
       "          6.03752801e-01,  4.24892240e-01, -1.36169612e-01, -1.45348247e-01,\n",
       "         -1.09832949e-01,  3.83587563e-06,  6.44539257e-08, -3.83126585e-18,\n",
       "         -1.91079569e-16, -5.11864729e-14,  5.26138825e-06,  7.50031136e-08,\n",
       "         -2.46153366e-05, -1.29834876e-17,  2.94396616e-16, -6.32178674e-19,\n",
       "         -5.26138825e-06, -7.50031136e-08,  2.46214240e-05, -9.88156877e-07,\n",
       "          7.65319420e-08]),\n",
       "  'achieved_goal': array([1.20576514, 0.6037528 , 0.42489224]),\n",
       "  'desired_goal': array([1.29409416, 0.78230083, 0.42489224])},\n",
       " {})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c378e55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/christomathew/PycharmProjects/LLM-Robotic-Arm/.venv/lib/python3.12/site-packages/gymnasium',\n",
       " '/Users/christomathew/miniforge3/envs/drl_env/lib/python3.12')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "os.path.dirname(gym.__file__), os.path.dirname(os.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e805c39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ActiveObjectWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Wraps a Fetch-style GoalEnv with multiple objects named object0..object4.\n",
    "\n",
    "    active_object_index:\n",
    "        which object index to treat as the task object.\n",
    "        0 means original object0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, default_index: int = 0, n_objects: int = 5):\n",
    "        super().__init__(env)\n",
    "        self.n_objects = n_objects\n",
    "        self.active_object_index = default_index\n",
    "\n",
    "    def set_active_object(self, idx: int):\n",
    "        \"\"\"Change which object is used as 'achieved_goal'.\"\"\"\n",
    "        if not 0 <= idx < self.n_objects:\n",
    "            raise ValueError(f\"active object index must be in [0, {self.n_objects - 1}]\")\n",
    "        self.active_object_index = idx\n",
    "\n",
    "    def _get_object_pos(self):\n",
    "        \"\"\"Read position of the currently active object from MuJoCo.\"\"\"\n",
    "        obj_name = f\"object{self.active_object_index}\"\n",
    "        # use site if you made sites 'object0'..'object4'\n",
    "        pos = self.env.sim.data.get_site_xpos(obj_name).copy()\n",
    "        # if you only have bodies, use:\n",
    "        # pos = self.env.sim.data.get_body_xpos(obj_name).copy()\n",
    "        return pos\n",
    "\n",
    "    def _remap_obs(self, obs):\n",
    "        \"\"\"Replace achieved_goal with selected object's position.\"\"\"\n",
    "        pos = self._get_object_pos()\n",
    "        obs[\"achieved_goal\"] = pos\n",
    "        return obs\n",
    "\n",
    "    def reset(self, *, active_object_id=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Optionally pass active_object_id at reset time.\n",
    "        Objects are already randomized by the env's _reset_sim.\n",
    "        \"\"\"\n",
    "        if active_object_id is not None:\n",
    "            self.set_active_object(active_object_id)\n",
    "\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        obs = self._remap_obs(obs)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # replace achieved_goal with selected object position\n",
    "        obs = self._remap_obs(obs)\n",
    "\n",
    "        # recompute reward based on the new achieved_goal\n",
    "        ag = obs[\"achieved_goal\"]\n",
    "        dg = obs[\"desired_goal\"]\n",
    "        # info can be anything, env.compute_reward ignores or uses it\n",
    "        reward = self.env.compute_reward(ag, dg, info)\n",
    "\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04177d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_fetch_env.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import gymnasium as gym \n",
    "import gymnasium_robotics\n",
    "from gymnasium_robotics.envs.fetch import MujocoFetchEnv\n",
    "from gymnasium.utils.ezpickle import EzPickle\n",
    "\n",
    "GYMROB_FILE = os.path.dirname(gymnasium_robotics.__file__)\n",
    "MODEL_XML_PATH = os.path.join(\n",
    "    GYMROB_FILE,\n",
    "    \"envs\",\n",
    "    \"assets\", \n",
    "    \"fetch\", \n",
    "    \"multi_pick_and_place.xml\"\n",
    ")\n",
    "\n",
    "\n",
    "def sample_non_overlapping_xy(\n",
    "    center_xy, \n",
    "    obj_range, \n",
    "    n_objects,\n",
    "    min_dist,\n",
    "    rng, \n",
    "    min_gripper_dist=0.10,\n",
    "    max_tries = 1000\n",
    "):\n",
    "    \n",
    "    positions = []\n",
    "    tries = 0\n",
    "    \n",
    "    while len(positions)<n_objects and tries < max_tries :\n",
    "        candidate = center_xy + rng.uniform(-obj_range, obj_range, size=2)\n",
    "        \n",
    "        # keep away from gripper\n",
    "        if np.linalg.norm(candidate - center_xy) < min_gripper_dist:\n",
    "            tries += 1\n",
    "            continue\n",
    "        \n",
    "        # keep away from other objects\n",
    "        ok = True\n",
    "        for p in positions:\n",
    "            if np.linalg.norm(candidate - p) < min_dist:\n",
    "                ok = False\n",
    "                break\n",
    "                \n",
    "        \n",
    "        if ok:\n",
    "            positions.append(candidate)\n",
    "        tries += 1\n",
    "            \n",
    "    if len(positions) < n_objects:\n",
    "        raise RuntimeError(\"Could not sample non-overlapping object positions. Reduce min_dist or increase obj_range.\")\n",
    "    return positions\n",
    "    \n",
    "class MultiObjectFetchPickAndPlaceEnv(MujocoFetchEnv, EzPickle):\n",
    "    def __init__(self, reward_type = 'sparse',n_objects=5,  **kwargs):\n",
    "        initial_qpos = {\n",
    "            \"robot0:slide0\": 0.405,\n",
    "            \"robot0:slide1\": 0.48,\n",
    "            \"robot0:slide2\": 0.0,\n",
    "            \"object0:joint\": [1.25, 0.53, 0.4, 1.0, 0.0, 0.0, 0.0],\n",
    "            \"object1:joint\": [1.25, 0.53, 0.4, 1.0, 0.0, 0.0, 0.0],\n",
    "            \"object2:joint\": [1.25, 0.53, 0.4, 1.0, 0.0, 0.0, 0.0],\n",
    "            \"object3:joint\": [1.25, 0.53, 0.4, 1.0, 0.0, 0.0, 0.0],\n",
    "            \"object4:joint\": [1.25, 0.53, 0.4, 1.0, 0.0, 0.0, 0.0],\n",
    "            \n",
    "        }\n",
    "        MujocoFetchEnv.__init__(\n",
    "                self,\n",
    "                model_path = MODEL_XML_PATH, \n",
    "                has_object=True, \n",
    "                block_gripper=False,\n",
    "                n_substeps=20,\n",
    "                gripper_extra_height=0.2,\n",
    "                target_in_the_air=True,\n",
    "                target_offset=0.0,\n",
    "                obj_range=0.15,\n",
    "                target_range=0.15,\n",
    "                distance_threshold=0.05,\n",
    "                initial_qpos=initial_qpos,\n",
    "                reward_type=reward_type,\n",
    "                **kwargs,\n",
    "        )\n",
    "        EzPickle.__init__(self, reward_type=reward_type, n_objects=n_objects, **kwargs)\n",
    "        self.n_objects = n_objects\n",
    "        \n",
    "    def _reset_sim(self):\n",
    "        # Reset buffers for joint states, actuators, warm-start, control buffers etc.\n",
    "        self._mujoco.mj_resetData(self.model, self.data)\n",
    "\n",
    "        self.data.time = self.initial_time\n",
    "        self.data.qpos[:] = np.copy(self.initial_qpos)\n",
    "        self.data.qvel[:] = np.copy(self.initial_qvel)\n",
    "        if self.model.na != 0:\n",
    "            self.data.act[:] = None\n",
    "            \n",
    "        # Randomize start position of object.\n",
    "        if self.has_object:\n",
    "            \n",
    "            center_xy = self.initial_gripper_xpos[:2].copy()\n",
    "            xy_list = sample_non_overlapping_xy(\n",
    "                center_xy=center_xy, \n",
    "                obj_range=self.obj_range, \n",
    "                n_objects=self.n_objects,\n",
    "                min_dist=self.distance_threshold, \n",
    "                rng = self.np_random,\n",
    "                max_tries=1000\n",
    "            )\n",
    "            \n",
    "            # Table height from object0 default (computed in _env_setup)\n",
    "            z = float(self.height_offset)\n",
    "        \n",
    "            for i, xy in enumerate(xy_list):\n",
    "                x, y = xy\n",
    "                joint_name = f\"object{i}:joint\"\n",
    "                object_qpos = self._utils.get_joint_qpos(self.model, self.data, joint_name)\n",
    "                assert object_qpos.shape == (7,)\n",
    "                object_qpos[0] = x\n",
    "                object_qpos[1] = y\n",
    "                object_qpos[2] = z\n",
    "                # keep orientation as-is (upright), or set quaternion explicitly if you want\n",
    "                self._utils.set_joint_qpos(\n",
    "                    self.model, self.data, joint_name, object_qpos \n",
    "                )\n",
    "\n",
    "        # 3) Forward physics\n",
    "        self._mujoco.mj_forward(self.model, self.data)\n",
    "        return True\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b665ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from multi_fetch_env import MultiObjectFetchPickAndPlaceEnv\n",
    "\n",
    "# env = MultiObjectFetchPickAndPlaceEnv(render_mode=\"human\")\n",
    "# obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "468bef9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'observation': array([ 1.34193475e+00,  7.49101049e-01,  5.34725189e-01,  1.47273946e+00,\n",
       "          7.95788880e-01,  4.54922386e-01,  1.30804706e-01,  4.66878310e-02,\n",
       "         -7.98028028e-02,  3.83587563e-06,  6.44539257e-08,  1.09764540e-02,\n",
       "         -4.36282865e-02,  1.04072756e-02, -7.31090126e-02, -8.82949285e-05,\n",
       "         -7.19913982e-02,  5.18577947e-05, -1.25141568e-04,  4.97140072e-05,\n",
       "         -5.26138825e-06, -7.50031136e-08,  2.46214240e-05, -9.88156877e-07,\n",
       "          7.65319420e-08]),\n",
       "  'achieved_goal': array([1.47273946, 0.79578888, 0.45492239]),\n",
       "  'desired_goal': array([1.49139962, 0.66888359, 0.8966153 ])},\n",
       " {})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db125f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6e288017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ActiveObjectWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Wraps a Fetch-style GoalEnv with multiple objects named object0..object4.\n",
    "\n",
    "    active_object_index:\n",
    "        which object index to treat as the task object.\n",
    "        0 means original object0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, default_index: int = 0, n_objects: int = 5):\n",
    "        super().__init__(env)\n",
    "        self.n_objects = n_objects\n",
    "        self.active_object_index = default_index\n",
    "\n",
    "    def set_active_object(self, idx: int):\n",
    "        \"\"\"Change which object is used as 'achieved_goal'.\"\"\"\n",
    "        if idx<0 or self.n_objects-1<idx:\n",
    "            raise ValueError(f\"Object index must be within the range 0 to {self.n_objects-1}\")\n",
    "        self.active_object_index = idx\n",
    "        \n",
    "\n",
    "    def _get_object_pos(self):\n",
    "        \"\"\"Read position of the currently active object from MuJoCo.\"\"\"\n",
    "        object_name = f\"object{self.active_object_index}\"\n",
    "        object_pos = self.env._utils.get_site_xpos( self.env.model, self.env.data, object_name)\n",
    "        return object_pos\n",
    "        \n",
    "    def _remap_obs(self, obs):\n",
    "        \"\"\"Replace achieved_goal with selected object's position.\"\"\"\n",
    "        obs['achieved_goal'] = self._get_object_pos()\n",
    "        return obs\n",
    "\n",
    "    def reset(self, *, active_object_id=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Optionally pass active_object_id at reset time.\n",
    "        Objects are already randomized by the env's _reset_sim.\n",
    "        \"\"\"\n",
    "        if active_object_id is not None:\n",
    "            self.set_active_object(active_object_id)\n",
    "            \n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        obs = self._remap_obs(obs)\n",
    "        return obs, info\n",
    "        \n",
    "    def step(self, action):\n",
    "        obs, reward, trunc, term, info = self.env.step(action)\n",
    "        \n",
    "        obs = self._remap_obs(obs)\n",
    "        a_g = obs['achieved_goal']\n",
    "        t_g = obs['desired_goal']\n",
    "        \n",
    "        reward = self.env.compute_reward(a_g, t_g, info)\n",
    "        \n",
    "        info[\"is_success\"] = self.env._is_success(a_g, t_g)\n",
    "        \n",
    "        return obs, reward, trunc, term, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ce438625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multi_fetch_env import MultiObjectFetchPickAndPlaceEnv\n",
    "\n",
    "env = MultiObjectFetchPickAndPlaceEnv(render_mode=\"human\")\n",
    "env = ActiveObjectWrapper(env, 0)\n",
    "obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8fff4c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observation': array([ 1.34193475e+00,  7.49101049e-01,  5.34725189e-01,  1.27766985e+00,\n",
       "         8.48421594e-01,  4.54922386e-01, -6.42649011e-02,  9.93205454e-02,\n",
       "        -7.98028028e-02,  3.83587563e-06,  6.44539257e-08,  1.09764540e-02,\n",
       "        -4.36282865e-02,  1.04072756e-02, -7.31090126e-02, -8.82949285e-05,\n",
       "        -7.19913982e-02,  5.18577947e-05, -1.25141568e-04,  4.97140072e-05,\n",
       "        -5.26138825e-06, -7.50031136e-08,  2.46214240e-05, -9.88156877e-07,\n",
       "         7.65319420e-08]),\n",
       " 'achieved_goal': array([1.27766985, 0.84842159, 0.45492239]),\n",
       " 'desired_goal': array([1.25680035, 0.64984908, 0.53460388])}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "16314531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observation': array([ 1.34221753e+00,  7.32247006e-01,  5.07637729e-01,  1.23434126e+00,\n",
       "         8.48556203e-01,  4.21328691e-01, -1.07876272e-01,  1.16309198e-01,\n",
       "        -8.63090380e-02,  2.62543825e-04,  2.79466891e-04,  1.87776364e-03,\n",
       "        -1.00495698e-02,  1.00625282e-02, -4.62512298e-03,  2.83196542e-03,\n",
       "         1.58697323e-02, -2.76270440e-03,  1.40567234e-02, -2.51623162e-06,\n",
       "         4.88676204e-03, -2.77071191e-03, -1.08502708e-02, -2.24994492e-04,\n",
       "         2.60229594e-04]),\n",
       " 'achieved_goal': array([1.23434126, 0.8485562 , 0.42132869]),\n",
       " 'desired_goal': array([1.25680035, 0.64984908, 0.53460388])}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, rew, term, trunc, info = env.step(env.action_space.sample())\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e0f5b8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observation': array([ 1.29448436e+00,  7.76726607e-01,  4.49494612e-01,  1.23456787e+00,\n",
       "         8.48600300e-01,  4.24883280e-01, -5.99164957e-02,  7.18736928e-02,\n",
       "        -2.46113317e-02,  5.16891807e-02,  5.16287967e-02,  3.66905812e-06,\n",
       "        -2.05061683e-05,  1.00527260e-02, -8.96794323e-03,  2.15176599e-02,\n",
       "         1.49021824e-02, -5.67877630e-06,  3.17311585e-05,  7.29709114e-14,\n",
       "         8.96874831e-03, -2.15175158e-02, -1.48889377e-02, -4.70625186e-03,\n",
       "        -4.53389253e-03]),\n",
       " 'achieved_goal': array([1.39842516, 0.86695123, 0.42488275]),\n",
       " 'desired_goal': array([1.25680035, 0.64984908, 0.53460388])}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env.set_active_object(1)\n",
    "obs, rew, term, trunc, info = env.step(env.action_space.sample())\n",
    "obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b5cec20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observation': array([ 1.26827985e+00,  7.45320103e-01,  4.22749553e-01,  1.23456827e+00,\n",
       "         8.48600372e-01,  4.24890074e-01, -3.37115820e-02,  1.03280268e-01,\n",
       "         2.14052109e-03,  5.01497040e-02,  5.01458055e-02,  8.41629936e-07,\n",
       "        -4.70461482e-06,  1.00527260e-02,  2.38741004e-02,  2.21864572e-02,\n",
       "         1.97117634e-02, -1.27561063e-06,  7.13012399e-06,  1.00521627e-15,\n",
       "        -2.38739197e-02, -2.21864248e-02, -1.97085829e-02, -3.11979707e-04,\n",
       "        -3.00315004e-04]),\n",
       " 'achieved_goal': array([1.47152741, 0.78837976, 0.42488992]),\n",
       " 'desired_goal': array([1.25680035, 0.64984908, 0.53460388])}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.set_active_object(2)\n",
    "obs, rew, term, trunc, info = env.step(env.action_space.sample())\n",
    "obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f2dc47",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677428f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1afc3bd8",
   "metadata": {},
   "source": [
    "### eval script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c224cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
