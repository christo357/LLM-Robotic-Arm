{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea1dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from sb3_contrib import TQC\n",
    "import my_envs\n",
    "from wrappers import ActiveObjectWrapper, ManualGoalWrapper\n",
    "\n",
    "gym.register_envs(gymnasium_robotics)\n",
    "\n",
    "LOG_DIR = 'logs_testing'\n",
    "# RUN_ID = os.getenv('RESUME_ID', None)\n",
    "# assert RUN_ID is not None,  \"Set RESUME_ID to the run folder name (e.g., 08vger36)\"\n",
    "\n",
    "ENV_ID = 'MultiObjectFetchPickAndPlace-v0'\n",
    "# MultiObjectFetchPickAndPlace-v0\n",
    "\n",
    "# ENV_ID = 'FetchPickAndPlace-v4'\n",
    "MODEL_PATH = os.path.join('models', 'tqcdense_model.zip')\n",
    "VECNORM_PATH = os.path.join('models', 'tqcdense_vecnorm.pkl')\n",
    "VIDEO_DIR = os.path.join(LOG_DIR, \"inference_videos\")\n",
    "\n",
    "\n",
    "def make_eval_env():\n",
    "    env = gym.make(\n",
    "        ENV_ID,\n",
    "        render_mode = 'human', \n",
    "        reward_type = 'dense', \n",
    "        n_objects = 4\n",
    "    )\n",
    "    # os.makedirs(VIDEO_DIR, exist_ok=True)\n",
    "\n",
    "    # env = RecordVideo(\n",
    "    #     env,\n",
    "    #     video_folder=VIDEO_DIR,\n",
    "    #     episode_trigger=lambda ep: True,  # record every episode\n",
    "    #     name_prefix=\"eval\",\n",
    "    # )\n",
    "    env = ActiveObjectWrapper(env, 0, 4)\n",
    "    env = ManualGoalWrapper(env)\n",
    "    # env = Monitor(env)  # for episode stats\n",
    "    \n",
    "    return env\n",
    "\n",
    "eval_env = DummyVecEnv([make_eval_env])\n",
    "\n",
    "if os.path.exists(VECNORM_PATH):\n",
    "    eval_env = VecNormalize.load(VECNORM_PATH, eval_env)\n",
    "else:\n",
    "    raise FileNotFoundError(f'Missing Vecnormalize file : {VECNORM_PATH}')\n",
    "\n",
    "eval_env.training = False\n",
    "eval_env.norm_reward = False\n",
    "\n",
    "model = TQC.load(MODEL_PATH, env = eval_env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe779ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('achieved_goal',\n",
       "              array([[-1.380873  , -1.2845722 , -0.05878199]], dtype=float32)),\n",
       "             ('desired_goal',\n",
       "              array([[ 0.36231285,  0.58025473, -0.57196546]], dtype=float32)),\n",
       "             ('observation',\n",
       "              array([[-2.8229108e-02, -2.3284504e-02, -1.3531129e-01, -1.3808730e+00,\n",
       "                      -1.2845722e+00, -5.8781985e-02, -8.5368133e-01, -7.1308279e-01,\n",
       "                       9.3479499e-02, -1.7087260e+00, -1.7319595e+00,  2.1000132e-02,\n",
       "                      -1.9299747e-01,  3.0267995e-02, -5.7901917e+00, -4.3915128e-03,\n",
       "                      -5.5173841e+00, -3.8756887e-04, -8.6273644e-03,  2.2135344e-03,\n",
       "                      -4.5994865e-03, -3.0075747e-03, -6.9535203e-02, -4.4036992e-02,\n",
       "                      -4.2243820e-02]], dtype=float32))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f6aaa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282cdf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set manual goal\n",
    "eval_env.env_method(\"set_goal_relative_to_object\", 1, (0.0, 0.0, 0.05))\n",
    "\n",
    "# IMPORTANT: refresh obs so desired_goal matches the new goal\n",
    "obs, _, _, _ = eval_env.step(np.array([[0.0, 0.0, 0.0, 0.0]], dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fce276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2b7c66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target reached\n",
      "Episode 1/1 return: -7.721\n"
     ]
    }
   ],
   "source": [
    "# 5) Run a few episodes\n",
    "n_episodes = 1\n",
    "for ep in range(n_episodes):\n",
    "    obs = eval_env.reset()\n",
    "    done = False\n",
    "    ep_return = 0.0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, dones, infos = eval_env.step(action)\n",
    "        ep_return += float(reward[0])\n",
    "        done = bool(dones[0])\n",
    "        time.sleep(.05)\n",
    "        info = infos[0]\n",
    "        if info.get(\"is_success\", False):\n",
    "            print(\"Target reached\")\n",
    "            break\n",
    "\n",
    "    print(f\"Episode {ep+1}/{n_episodes} return: {ep_return:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "553889f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "action[0,3] = 1\n",
    "obs, _, _, _ = eval_env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f888a401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('achieved_goal',\n",
       "              array([[ 1.0666339 ,  1.1523316 , -0.05878199]], dtype=float32)),\n",
       "             ('desired_goal',\n",
       "              array([[-0.94194406,  0.8088951 , -0.57196546]], dtype=float32)),\n",
       "             ('observation',\n",
       "              array([[-2.8229108e-02, -2.3284504e-02, -1.3531129e-01,  1.0666339e+00,\n",
       "                       1.1523316e+00, -5.8781985e-02,  7.0643717e-01,  6.8221384e-01,\n",
       "                       9.3479499e-02, -1.7087260e+00, -1.7319595e+00,  3.7060216e-02,\n",
       "                       6.8642974e-02,  2.2598559e-01,  2.8186576e+00,  6.0774949e-03,\n",
       "                      -5.6209073e+00,  7.6922867e-04, -5.5768914e-03,  1.7459443e-02,\n",
       "                      -4.5994865e-03, -3.0075747e-03, -6.9535203e-02, -4.4036992e-02,\n",
       "                      -4.2243820e-02]], dtype=float32))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_env.env_method(\"set_active_object\", 3)\n",
    "eval_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b3d2859",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de3f078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target reached\n",
      "Episode 1/1 return: -16.426\n"
     ]
    }
   ],
   "source": [
    "# 5) Run a few episodes\n",
    "n_episodes = 1\n",
    "for ep in range(n_episodes):\n",
    "    obs = eval_env.reset()\n",
    "    done = False\n",
    "    ep_return = 0.0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, dones, infos = eval_env.step(action)\n",
    "        ep_return += float(reward[0])\n",
    "        done = bool(dones[0])\n",
    "        time.sleep(.01)\n",
    "        info = infos[0]\n",
    "        if info.get(\"is_success\", False):\n",
    "            print(\"Target reached\")\n",
    "            break\n",
    "\n",
    "\n",
    "    print(f\"Episode {ep+1}/{n_episodes} return: {ep_return:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e454648f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'is_success': np.float32(0.0), 'TimeLimit.truncated': False}]\n"
     ]
    }
   ],
   "source": [
    "action, _ = model.predict(obs, deterministic=True)\n",
    "obs, reward, dones, infos = eval_env.step(action)\n",
    "print(infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68118ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#release the object\n",
    "action = np.array([[0.0, 0.0, 0.0, +1.0]])\n",
    "obs, reward, dones, infos = eval_env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23623398",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ddfafa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.38441518,  0.68080485, -0.38448015, -0.6948155 ], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57129da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
